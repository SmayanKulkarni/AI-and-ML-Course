{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ed38d9",
   "metadata": {},
   "source": [
    "## Creating Linear Regression from Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9341b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600229ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Steps:** \n",
    "1. Define function for prediction\n",
    "2. Define Cost Function\n",
    "3. Define function to calculate gradients\n",
    "4. Define function to get gradient descent\n",
    "5. Run prediction function on weights and bias found using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccd205",
   "metadata": {},
   "source": [
    "### Step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50290fb2",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "# Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6576b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, x):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to calculate the predictions for Linear Regression Model\n",
    "    Args:\n",
    "    x : Example with multiple features\n",
    "    b : model bias\n",
    "    w : model weights\n",
    "    Returns:\n",
    "        p (scalar): The prediction for the given inputs\n",
    "    \"\"\"\n",
    "    p = np.dot(x,w) + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a966e526",
   "metadata": {},
   "source": [
    "### Step 2: \n",
    "\n",
    "\n",
    "<a name=\"toc_15456_4\"></a>\n",
    "# Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b47e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w,b,x,y):\n",
    "    m = len(x)\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(w, x[i]) +b\n",
    "        cost = cost + (f_wb_i - y[i])**2\n",
    "    cost = cost/(2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd000e0",
   "metadata": {},
   "source": [
    "### Step 3 and 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe727e8",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "# Gradient Descent With Multiple Variables\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e382fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(w,b,x,y):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = x.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(x[i], w) + b\n",
    "        err = f_wb_i - y[i]\n",
    "\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err * x[i,j]\n",
    "        dj_db = dj_db + err\n",
    "    dj_db = dj_db /m\n",
    "    dj_dw = dj_dw /m\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86900ba",
   "metadata": {},
   "source": [
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd54bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def gradient_descent(x,y,w_in,b_in, alpha, num_iters, cost = cost, compute_gradient = compute_gradient):\n",
    "        \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost       : function to compute cost\n",
    "      compute_gradient   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "\n",
    "        w = copy.deepcopy(w_in)\n",
    "        b = b_in\n",
    "\n",
    "        for i in range(num_iters):\n",
    "            dj_db, dj_dw = compute_gradient(w,b,x,y)\n",
    "\n",
    "            w = w - alpha * dj_dw\n",
    "            b = b - alpha * dj_db\n",
    "\n",
    "        \n",
    "        return w,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e56fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6fcbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30e8b8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>distance_traveled</th>\n",
       "      <th>num_of_passengers</th>\n",
       "      <th>fare</th>\n",
       "      <th>tip</th>\n",
       "      <th>miscellaneous_fees</th>\n",
       "      <th>total_fare</th>\n",
       "      <th>surge_applied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>748.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.00</td>\n",
       "      <td>24</td>\n",
       "      <td>6.300</td>\n",
       "      <td>105.300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1187.0</td>\n",
       "      <td>3.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105.00</td>\n",
       "      <td>24</td>\n",
       "      <td>13.200</td>\n",
       "      <td>142.200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>730.0</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.25</td>\n",
       "      <td>0</td>\n",
       "      <td>26.625</td>\n",
       "      <td>97.875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>671.0</td>\n",
       "      <td>5.63</td>\n",
       "      <td>3.0</td>\n",
       "      <td>90.00</td>\n",
       "      <td>0</td>\n",
       "      <td>9.750</td>\n",
       "      <td>99.750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>329.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>12</td>\n",
       "      <td>13.200</td>\n",
       "      <td>70.200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_duration  distance_traveled  num_of_passengers    fare  tip  \\\n",
       "0          748.0               2.75                1.0   75.00   24   \n",
       "1         1187.0               3.43                1.0  105.00   24   \n",
       "2          730.0               3.12                1.0   71.25    0   \n",
       "3          671.0               5.63                3.0   90.00    0   \n",
       "4          329.0               2.09                1.0   45.00   12   \n",
       "\n",
       "   miscellaneous_fees  total_fare  surge_applied  \n",
       "0               6.300     105.300              0  \n",
       "1              13.200     142.200              0  \n",
       "2              26.625      97.875              1  \n",
       "3               9.750      99.750              0  \n",
       "4              13.200      70.200              0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15e2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('total_fare', axis = 1).to_numpy()\n",
    "y =df['total_fare'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27756801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=234, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "645dbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "210cef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b4bf4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_in = np.zeros((n,))\n",
    "b_in = 3.23423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "188bceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_final, b_final = gradient_descent(x_train_scaled,y_train,w_in,b_in, alpha=0.01, num_iters=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef04cad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.96466143e-02, -5.72482812e-04,  6.84564677e-04,  8.64246015e+01,\n",
       "         2.09789179e+01,  1.27920267e+01, -2.15199371e-01]),\n",
       " 127.85254068247805)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_final, b_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41e280bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_model = predict(w_final, b_final, x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad8dcd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score is:  0.999996876245515\n",
      "Mean Absolute Error Score is:  0.09662777434687678\n",
      "Mean Squared Error Score is:  0.029067723008694302\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "print(\"R2 Score is: \", r2_score(y_test, y_preds_model))\n",
    "print(\"Mean Absolute Error Score is: \", mean_absolute_error(y_test, y_preds_model))\n",
    "print(\"Mean Squared Error Score is: \", mean_squared_error(y_test, y_preds_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab31de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(x_train_scaled, y_train)\n",
    "\n",
    "y_preds = lr.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57e2cfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score is:  1.0\n",
      "Mean Absolute Error Score is:  9.30521226614631e-14\n",
      "Mean Squared Error Score is:  1.543381464766739e-26\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 Score is: \", r2_score(y_test, y_preds))\n",
    "print(\"Mean Absolute Error Score is: \", mean_absolute_error(y_test, y_preds))\n",
    "print(\"Mean Squared Error Score is: \", mean_squared_error(y_test, y_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
